{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression?\n",
        "\n",
        "Simple Linear Regression is a statistical method that models the relationship between one independent variable (X) and one dependent variable (Y) using a straight line.\n",
        "\n",
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "Linearity\n",
        "\n",
        "Independence of errors\n",
        "\n",
        "Homoscedasticity (constant error variance)\n",
        "\n",
        "Normality of residuals\n",
        "\n",
        "No significant outliers\n",
        "\n",
        "3. What does the coefficient m represent in the equation Y = mX + c?\n",
        "\n",
        "The coefficient m is the slope, representing how much Y changes for a one-unit increase in X.\n",
        "\n",
        "4. What does the intercept c represent in the equation Y = mX + c?\n",
        "\n",
        "The intercept c is the value of Y when X = 0, i.e., the starting point of the regression line.\n",
        "\n",
        "5. How do we calculate the slope m in Simple Linear Regression?\n",
        "ğ‘š\n",
        "=\n",
        "âˆ‘\n",
        "(\n",
        "ğ‘‹\n",
        "âˆ’\n",
        "ğ‘‹\n",
        "Ë‰\n",
        ")\n",
        "(\n",
        "ğ‘Œ\n",
        "âˆ’\n",
        "ğ‘Œ\n",
        "Ë‰\n",
        ")\n",
        "âˆ‘\n",
        "(\n",
        "ğ‘‹\n",
        "âˆ’\n",
        "ğ‘‹\n",
        "Ë‰\n",
        ")\n",
        "2\n",
        "m=\n",
        "âˆ‘(Xâˆ’\n",
        "X\n",
        "Ë‰\n",
        ")\n",
        "2\n",
        "âˆ‘(Xâˆ’\n",
        "X\n",
        "Ë‰\n",
        ")(Yâˆ’\n",
        "Y\n",
        "Ë‰\n",
        ")\n",
        "\tâ€‹\n",
        "\n",
        "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "\n",
        "The least squares method finds the best-fit line by minimizing the sum of squared differences between actual and predicted values.\n",
        "\n",
        "7. How is the coefficient of determination (RÂ²) interpreted in Simple Linear Regression?\n",
        "\n",
        "RÂ² shows the percentage of variation in Y explained by X.\n",
        "\n",
        "RÂ² = 0 â†’ no explanatory power\n",
        "\n",
        "RÂ² = 1 â†’ perfect explanation\n",
        "\n",
        "8. What is Multiple Linear Regression?\n",
        "\n",
        "Multiple Linear Regression models the relationship between two or more independent variables and a dependent variable.\n",
        "\n",
        "9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "\n",
        "Simple Regression: one independent variable\n",
        "\n",
        "Multiple Regression: two or more independent variables\n",
        "\n",
        "10. What are the key assumptions of Multiple Linear Regression?\n",
        "\n",
        "Linearity\n",
        "\n",
        "Independence of residuals\n",
        "\n",
        "Homoscedasticity\n",
        "\n",
        "Normality of residuals\n",
        "\n",
        "No multicollinearity\n",
        "\n",
        "No significant outliers\n",
        "\n",
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "\n",
        "Heteroscedasticity means unequal variance of residuals.\n",
        "It causes:\n",
        "\n",
        "Inefficient estimates\n",
        "\n",
        "Biased standard errors\n",
        "\n",
        "Incorrect hypothesis tests\n",
        "\n",
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        "Remove highly correlated predictors\n",
        "\n",
        "Use Ridge or Lasso regression\n",
        "\n",
        "Use PCA (dimensionality reduction)\n",
        "\n",
        "Combine correlated variables\n",
        "\n",
        "13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        "One-hot encoding\n",
        "\n",
        "Dummy variables\n",
        "\n",
        "Label encoding\n",
        "\n",
        "Ordinal encoding\n",
        "\n",
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        "Interaction terms measure combined effects of two variables on Y, showing how one variableâ€™s effect depends on another.\n",
        "\n",
        "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "\n",
        "Simple: Y when X = 0\n",
        "\n",
        "Multiple: Y when all predictors = 0, which may not always be meaningful\n",
        "\n",
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "\n",
        "The slope shows how much Y changes for each unit increase in X.\n",
        "A larger slope â†’ stronger effect on predictions.\n",
        "\n",
        "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "\n",
        "It provides the baseline value of Y, helping understand the starting point before X influences the outcome.\n",
        "\n",
        "18. What are the limitations of using RÂ² as a sole measure of model performance?\n",
        "\n",
        "Increases even with irrelevant variables\n",
        "\n",
        "Does not indicate overfitting\n",
        "\n",
        "Cannot compare models with different dependent variables\n",
        "\n",
        "Does not evaluate model correctness\n",
        "\n",
        "19. How would you interpret a large standard error for a regression coefficient?\n",
        "\n",
        "Coefficient estimate is unstable\n",
        "\n",
        "Variable may be insignificant\n",
        "\n",
        "Indicates high variance or multicollinearity\n",
        "\n",
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "\n",
        "Identification:\n",
        "\n",
        "Residuals show patterns (funnel shape, increasing spread)\n",
        "\n",
        "Importance:\n",
        "\n",
        "Leads to unreliable standard errors and hypothesis tests\n",
        "\n",
        "21. What does it mean if a Multiple Linear Regression model has a high RÂ² but low adjusted RÂ²?\n",
        "\n",
        "This means irrelevant variables are included, artificially increasing RÂ² but penalized in adjusted RÂ².\n",
        "\n",
        "22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "\n",
        "Ensures equal influence of features\n",
        "\n",
        "Improves model stability\n",
        "\n",
        "Essential for regularization\n",
        "\n",
        "Helps convergence of algorithms\n",
        "\n",
        "23. What is polynomial regression?\n",
        "\n",
        "Polynomial regression fits a curved relationship between X and Y using powers of X.\n",
        "\n",
        "24. How does polynomial regression differ from linear regression?\n",
        "\n",
        "Linear regression: straight-line relationship\n",
        "\n",
        "Polynomial regression: curved relationship using\n",
        "ğ‘‹\n",
        "2\n",
        ",\n",
        "ğ‘‹\n",
        "3\n",
        ".\n",
        ".\n",
        ".\n",
        "X\n",
        "2\n",
        ",X\n",
        "3\n",
        "...\n",
        "\n",
        "25. When is polynomial regression used?\n",
        "\n",
        "When the data shows a nonlinear pattern that cannot be captured by a straight line.\n",
        "\n",
        "26. What is the general equation for polynomial regression?\n",
        "ğ‘Œ\n",
        "=\n",
        "ğ‘\n",
        "0\n",
        "+\n",
        "ğ‘\n",
        "1\n",
        "ğ‘‹\n",
        "+\n",
        "ğ‘\n",
        "2\n",
        "ğ‘‹\n",
        "2\n",
        "+\n",
        "â‹¯\n",
        "+\n",
        "ğ‘\n",
        "ğ‘›\n",
        "ğ‘‹\n",
        "ğ‘›\n",
        "Y=b\n",
        "0\n",
        "\tâ€‹\n",
        "\n",
        "+b\n",
        "1\n",
        "\tâ€‹\n",
        "\n",
        "X+b\n",
        "2\n",
        "\tâ€‹\n",
        "\n",
        "X\n",
        "2\n",
        "+â‹¯+b\n",
        "n\n",
        "\tâ€‹\n",
        "\n",
        "X\n",
        "n\n",
        "27. Can polynomial regression be applied to multiple variables?\n",
        "\n",
        "Yes, it becomes multivariate polynomial regression, involving powers and interactions of multiple predictors.\n",
        "\n",
        "28. What are the limitations of polynomial regression?\n",
        "\n",
        "High risk of overfitting\n",
        "\n",
        "Hard to interpret\n",
        "\n",
        "Unstable for high degrees\n",
        "\n",
        "Poor extrapolation\n",
        "\n",
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "\n",
        "Cross-validation\n",
        "\n",
        "Adjusted RÂ²\n",
        "\n",
        "AIC/BIC\n",
        "\n",
        "Validation set performance\n",
        "\n",
        "Residual analysis\n",
        "\n",
        "30. Why is visualization important in polynomial regression?\n",
        "\n",
        "Visualization helps confirm:\n",
        "\n",
        "Fit of the curve\n",
        "\n",
        "Overfitting or underfitting\n",
        "\n",
        "Whether the polynomial shape matches the data trend\n",
        "\n",
        "31. How is polynomial regression implemented in Python?\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "import numpy as np\n",
        "\n",
        "# Example data\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "y = np.array([2, 5, 10, 17, 26])\n",
        "\n",
        "# Polynomial features (degree 2)\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Fit model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_poly)\n",
        "print(y_pred)\n"
      ],
      "metadata": {
        "id": "w1uL2J4Vjy9q"
      }
    }
  ]
}